<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>tbgs</title>
    <description>musings of three bored graduate students</description>
    <link>http://localhost:4000/blog/</link>
    <atom:link href="http://localhost:4000/blog/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 11 May 2018 13:46:15 -0700</pubDate>
    <lastBuildDate>Fri, 11 May 2018 13:46:15 -0700</lastBuildDate>
    <generator>Jekyll v3.6.2</generator>
    
      <item>
        <title>Using a matrix equality for (small-scale) image classification</title>
        <description>&lt;p&gt;In this post I will walk through a concrete application of &lt;a href=&quot;http://people.eecs.berkeley.edu/~stephentu/blog/matrix-analysis/2016/06/03/matrix-inverse-equality.html&quot;&gt;a matrix equality&lt;/a&gt; to speed up the training process of a simple image classification pipeline.&lt;/p&gt;

&lt;h4 id=&quot;background&quot;&gt;Background&lt;/h4&gt;
&lt;p&gt;I have a relatively small collection of blurry images (32 x 32 rgb pixels) from the cifar data set (50,000 images) from each of 10 classes. The task is to build a
model to classify these images.&lt;/p&gt;

&lt;p&gt;For reference, the images look like this:&lt;/p&gt;
&lt;p&gt;
&lt;img src=&quot;/blog/assets/images/cifar_frog.png&quot; width=&quot;256&quot; id=&quot;cifar_frog&quot; /&gt;
&lt;/p&gt;

&lt;h4 id=&quot;problem-formulation&quot;&gt;Problem Formulation&lt;/h4&gt;
&lt;p&gt;We can represent each image as a vector in $\mathrm{R}^{32 \times 32 \times 3}$ (a 3072 dimensional vector).
Then we stack all these images into a $50000 \times 3072$ matrix and call it $X$&lt;/p&gt;

&lt;p&gt;We can let Y be a $50000 \times 10$ matrix of corresponding &lt;a href=&quot;http://stackoverflow.com/questions/17469835/one-hot-encoding-for-machine-learning&quot;&gt;one hot encoded&lt;/a&gt; image labels.&lt;/p&gt;

&lt;p&gt;We denote $X_{i}$ to be the $ith$ row of $X$ (or the $ith$ image)&lt;/p&gt;

&lt;p&gt;We denote $Y_{i}$ to be the $ith$ row of $Y$ (or the $ith$ image label)&lt;/p&gt;

&lt;p&gt;
Now the task is to build a &lt;i&gt;generalizable&lt;/i&gt; map from $X_i \to Y_i$
&lt;/p&gt;

&lt;h4 id=&quot;strawman&quot;&gt;Strawman&lt;/h4&gt;

&lt;p&gt;What is the simplest map we can think of?&lt;/p&gt;

&lt;p&gt;LINEAR!&lt;/p&gt;

&lt;p&gt;That is we want to find a matrix $W$ such that $xW$ is close to $y$, the label vector.&lt;/p&gt;

&lt;p&gt;Particularly we want the maximal index of $xW$ to be the same as the maximal index
of $y$.&lt;/p&gt;

&lt;p&gt;Another way to state this is that we want $||xW - y||_{2}^{2}$ to be small.&lt;/p&gt;

&lt;p&gt;We can formulate an optimization problem.&lt;/p&gt;

&lt;p&gt;
So lets try to minimize
$\frac{1}{2} \|XW - Y\|_{F}^{2} + \lambda \|W\|^{2}_{F}$
&lt;/p&gt;

&lt;p&gt;Note I added a &lt;em&gt;penalty&lt;/em&gt; term, this is very common.
In the derivation of the solution it will be clear why the penalty
term is necessary. Note the $_F$ simply means I will be using the &lt;a href=&quot;https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm&quot;&gt;Frobenius norm&lt;/a&gt;,
which means I’ll treat the matrices XW and Y as large vectors and use the standard euclidean
norm.&lt;/p&gt;

&lt;p&gt;Note:
$||X||_{F}^{2} = \mathrm{Tr}(X^{T}X)$&lt;/p&gt;

&lt;p&gt;Where $\mathrm{Tr}$ is the trace.&lt;/p&gt;

&lt;h4 id=&quot;strawman-solution&quot;&gt;Strawman solution&lt;/h4&gt;
&lt;p&gt;We can find the optimum solution with some matrix calculus:&lt;/p&gt;

&lt;p&gt;First expand&lt;/p&gt;

&lt;p&gt;$ \mathrm{Tr}(W^{T}X^{T}XW) - \mathrm{Tr}(X^{T}WY) + \mathrm{Tr}(Y^{T}Y)  + \lambda \mathrm{Tr}(W^{T}W)$&lt;/p&gt;

&lt;p&gt;Note I converted the Frobenius norm to a trace&lt;/p&gt;

&lt;p&gt;Then take derivative and set to 0.&lt;/p&gt;

&lt;p&gt;Note trace and derivative commute&lt;/p&gt;

&lt;p&gt;$ \mathrm{Tr}(X^{T}XW) - \mathrm{Tr}(X^{T}Y) + \lambda I_{d} \mathrm{Tr}(W) = 0$&lt;/p&gt;

&lt;p&gt;$ \mathrm{Tr}(X^{T}XW) +  \lambda \mathrm{Tr}(W) = \mathrm{Tr}(X^{T}Y)$&lt;/p&gt;

&lt;p&gt;Linearity of \mathrm{Tr}ace&lt;/p&gt;

&lt;p&gt;$ \mathrm{Tr}((X^{T}X +  I_{d}\lambda) W) = \mathrm{Tr}(X^{T}Y)$&lt;/p&gt;

&lt;p&gt;This is satisfied when:&lt;/p&gt;

&lt;p&gt;$W =  (X^{T}X +  I_{d}\lambda)^{-1}X^{T}Y$&lt;/p&gt;

&lt;p&gt;Which would be our optimum. Since $d$ in this case is only $3072$ this is a quick and easy computation. Note the penalty term makes our matrix invertible when $X^{T}X$
is singular.
that can be done in one line of python.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; d = X.shape[1]
&amp;gt;&amp;gt;&amp;gt; W = scipy.linalg.solve(X.dot(X) + lambdav*np.eye(d), X.T.dot(Y))
&amp;gt;&amp;gt;&amp;gt; predictedTestLabels = argmax(Xtest.dot(W), axis=1)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;####How did it do?&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; (predictedTestLabels == labels)/float(len(labels))
0.373
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;…Ouch&lt;/p&gt;

&lt;p&gt;####Strawman++&lt;/p&gt;

&lt;p&gt;Unfortunately the raw space  these image-vectors live in isn’t very good for
linear classification, so our model will perform poorly. So lets “lift” our data
to a “better” space.&lt;/p&gt;

&lt;p&gt;Let $\Phi$ be a featurization function, that will “lift” our data. I’ve
heard neural networks work well for this task, so I’ll let $\Phi$ be a convolutional neural net (cnn)&lt;/p&gt;

&lt;p&gt;I’m lazy so I don’t have time to add a lot of layers, so it’ll be a one layer CNN.&lt;/p&gt;

&lt;p&gt;Furthermore I’m really lazy so I’m not going to train the network.  So $\Phi$ is a
&lt;em&gt;random&lt;/em&gt;, &lt;em&gt;single layer&lt;/em&gt; convolutional neural network.&lt;/p&gt;

&lt;p&gt;
Specifically I used a network with $6 x 6$ patches, $1024$ filters, a RELU nonlinearity and a average pooler to $2 x 2$.
This will make the output dimension $4096$
&lt;/p&gt;

&lt;h4 id=&quot;how-did-it-do&quot;&gt;How did it do?&lt;/h4&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; A = phi(X, seed=0)
&amp;gt;&amp;gt;&amp;gt; d = A.shape[1]
&amp;gt;&amp;gt;&amp;gt; lambdav = 0.1
&amp;gt;&amp;gt;&amp;gt; W = scipy.linalg.solve(A.T.dot(A) + lambdav*np.eye(d), A.T.dot(Y))
&amp;gt;&amp;gt;&amp;gt; predictedTestLabels = argmax(phi(Xtest, seed=0).dot(W), axis=1)
&amp;gt;&amp;gt;&amp;gt; (predictedTestLabels == labels)/float(len(labels))
0.64
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Holy smokes batman!&lt;/p&gt;

&lt;p&gt;thats a big jump. But we can do better.&lt;/p&gt;

&lt;p&gt;####Tinman
Since our $\Phi$ is just a random map, what if we “lift” X
multiple times (independently) and concatenate those representations,
since each one is independent and random.&lt;/p&gt;

&lt;p&gt;Let $\Phi_{k}$ be this concatenation map&lt;/p&gt;

&lt;p&gt;That is:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; k = 25
&amp;gt;&amp;gt;&amp;gt; def phik(X):
        return np.hstack(map(lambdav i: phi(X, seed=i), range(k)))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can let $A = \Phi_{k}(X)$, note A is now $50000 \times 4096k$&lt;/p&gt;

&lt;p&gt;
Remember we want to minimize $\frac{1}{2}\|AW - Y\|_{F}^{2} + \lambda\|W\|_{F}^{2}$
&lt;/p&gt;

&lt;p&gt;Our previous calculus tells us $W^{*} = (A^{T}A + \lambda I_{4096k})^{-1}A^{T}Y$&lt;/p&gt;

&lt;p&gt;And our prediction vector would be $\hat{Y} = A(A^{T}A + \lambda I_{d})^{-1}A^{T}Y$&lt;/p&gt;

&lt;p&gt;Even for moderate values of k (perhaps over $25$), $(AA^{T} + \lambda I_{d})^{-1}$ becomes very hard to compute (since the inverse scales as $d^{3}$).&lt;/p&gt;

&lt;p&gt;We can finally use the &lt;a href=&quot;https://people.eecs.berkeley.edu/~stephentu/blog/matrix-analysis/2016/06/03/matrix-inverse-equality.html&quot;&gt;useful matrix equality&lt;/a&gt;
to rewrite the prediction vector&lt;/p&gt;

&lt;p&gt;$\hat{Y} = AA^{T}(AA^{T} + \lambda I_{50000})^{-1}Y$&lt;/p&gt;

&lt;p&gt;Thus our new linear model looks like:&lt;/p&gt;

&lt;p&gt;$W = A^{T}(AA^{T} + \lambda I_{50000})^{-1}Y$&lt;/p&gt;

&lt;p&gt;Now we never have to invert anything greater than $50000 \times 50000$ !&lt;/p&gt;

&lt;p&gt;I’m going to try $k=25$&lt;/p&gt;

&lt;p&gt;####How did it do?&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; A = phik(X)
&amp;gt;&amp;gt;&amp;gt; W = A.t.dot(scipy.linalg.solve(A.dot(A.t) + lambdav * np.eye(n), Y, sym_pos=True))
&amp;gt;&amp;gt;&amp;gt; predictedTestLabels= np.argmax(phik(Xtest).dot(C), axis=1)
&amp;gt;&amp;gt;&amp;gt; (predictedTestLabels == labels)/float(len(labels))
0.75
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;yay!&lt;/p&gt;

&lt;h4 id=&quot;you-skipped-some-steps&quot;&gt;You skipped some steps!&lt;/h4&gt;

&lt;p&gt;There are a couple key details I left out of this post. Both are issues around
making the above method practical (even on small datasets like CIFAR-10).&lt;/p&gt;

&lt;p&gt;One is the actual efficient computation of $\Phi$, this step can
be easily parallelized or sped up using vector operations (or both).&lt;/p&gt;

&lt;p&gt;The actual observed behavior is that the test accuracy climbs as the number of
random features are accumulated, so we want to push $k$ as large as possible.
But we also want to avoid memory problems when $n \times d$ gets too large.
So we want to avoid materializing X.&lt;/p&gt;

</description>
        <pubDate>Wed, 20 Jul 2016 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/blog/ml/2016/07/20/matrix-inequality.html</link>
        <guid isPermaLink="true">http://localhost:4000/blog/ml/2016/07/20/matrix-inequality.html</guid>
        
        
        <category>ml</category>
        
      </item>
    
  </channel>
</rss>
